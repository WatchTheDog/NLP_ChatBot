{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tweets\n",
    "\n",
    "Donwload and save tweets, using a **query** value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('C:/Users/Timo/Source/Repos/NLP/NLP_ChatBot/.env').resolve()\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API access\n",
    "\n",
    "First of all, we'll connect to the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.getenv(\"CONSUMER_KEY\")\n",
    "consumer_secret = os.getenv(\"CONSUMER_SECRET\")\n",
    "access_token = os.getenv(\"ACCESS_TOKEN\")\n",
    "access_token_secret = os.getenv(\"ACCESS_TOKEN_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler,API,TweepError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the Twitter API.\n"
     ]
    }
   ],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = API(auth)\n",
    "print('Successfully connected to the Twitter API.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Tweets\n",
    "\n",
    "Now we can define our query and search for the tweets containing it.\n",
    "\n",
    "- **query**: *hashtag* or *emoji* that will be used to fetch the tweets\n",
    "- **max_requests**: Maximum number of requests to the API.\n",
    "    - Restriction: 180 requests / 15 min window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ':anxious_face_with_sweat:'\n",
    "max_requests = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts aliases to the real emoji representation (e.g. :thumbs_up: => üëç)\n",
    "\n",
    "from emoji import emojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = emojize(query) + ' -filter:retweets'\n",
    "searched_tweets = []\n",
    "last_id = -1\n",
    "request_count = 0\n",
    "while request_count < max_requests:\n",
    "    try:\n",
    "        new_tweets = api.search(q=q,\n",
    "                                lang='en',\n",
    "                                count=100,\n",
    "                                max_id=str(last_id - 1),\n",
    "                                tweet_mode='extended')\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        searched_tweets.extend(new_tweets)\n",
    "        last_id = new_tweets[-1].id\n",
    "        request_count += 1\n",
    "    except TweepError as e:\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and save\n",
    "\n",
    "Format the API data to the desired structure and save a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997 :anxious_face_with_sweat: tweets\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for tweet in searched_tweets:\n",
    "    data.append([tweet.id, tweet.created_at, tweet.user.screen_name, tweet.full_text])\n",
    "df = pd.DataFrame(data=data, columns=['id', 'date', 'user', 'text'])\n",
    "print(str(len(data)) + ' ' + query + ' tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    id                date             user  \\\n0  1335882489886371842 2020-12-07 09:42:59  watashi_no_maho   \n1  1335882444193591296 2020-12-07 09:42:48       ramendates   \n2  1335882402909007873 2020-12-07 09:42:39        shona_man   \n3  1335882402829361153 2020-12-07 09:42:38   JunesFavGemini   \n4  1335882380331143169 2020-12-07 09:42:33      LostinTokio   \n\n                                                text  \n0                     @notyourpil i totally forgot üò∞  \n1                           chungha get well soon üò∞üíõ  \n2                            @rejecteee16 üò∞yoo sorry  \n3  My contact stuck in my eye! Anybody know how t...  \n4  Going to see the specialist again about my bre...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1335882489886371842</td>\n      <td>2020-12-07 09:42:59</td>\n      <td>watashi_no_maho</td>\n      <td>@notyourpil i totally forgot üò∞</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1335882444193591296</td>\n      <td>2020-12-07 09:42:48</td>\n      <td>ramendates</td>\n      <td>chungha get well soon üò∞üíõ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1335882402909007873</td>\n      <td>2020-12-07 09:42:39</td>\n      <td>shona_man</td>\n      <td>@rejecteee16 üò∞yoo sorry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1335882402829361153</td>\n      <td>2020-12-07 09:42:38</td>\n      <td>JunesFavGemini</td>\n      <td>My contact stuck in my eye! Anybody know how t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1335882380331143169</td>\n      <td>2020-12-07 09:42:33</td>\n      <td>LostinTokio</td>\n      <td>Going to see the specialist again about my bre...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved under: \"C:/Users/Timo/Source/Repos/NLP/NLP_ChatBot/datasets/tweepy\"\n"
     ]
    }
   ],
   "source": [
    "PATH = Path('C:/Users/Timo/Source/Repos/NLP/NLP_ChatBot/datasets/tweepy').resolve()\n",
    "filename = query.replace(':', 'Z') + '.csv'\n",
    "df.to_csv(os.path.join(PATH, filename), index=None)\n",
    "print('Saved under: \"' + PATH.as_posix() + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-cdac4e92",
   "language": "python",
   "display_name": "PyCharm (NLP-Tennis-Bot-master)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}